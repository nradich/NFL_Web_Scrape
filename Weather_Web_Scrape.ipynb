{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping historical weather data for Hard Rock Stadium in Miami Gardens, Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dates = {\"Dates\": ['2017-9-17', '2017-9-24','2017-10-01', '2017-10-08',\\\n",
    "                    '2017-10-15', '2017-10-22', '2017-10-26', '2017-11-05',\\\n",
    "                  '2017-11-13', '2017-11-19', '2017-11-26', '2017-12-03',\\\n",
    "                  '2017-12-11', '2017-12-17', '2017-12-24', '2017-12-31',\\\n",
    "                   '2018-9-09', '2018-9-16','2018-9-23', '2018-9-30', \\\n",
    "                   '2018-10-07','2018-10-14', '2018-10-21', '2018-10-25', \\\n",
    "                   '2018-11-04', '2018-11-11','2018-11-25','2018-12-02',\\\n",
    "                   '2018-12-09', '2018-12-16', '2018-12-23', '2018-12-30'],\n",
    "        'Location': ['Away_San Diego', 'Away_New York', 'Home', 'Home', \\\n",
    "                     'Away_Atlanta', 'Home', 'Away_Baltimore','Home',\\\n",
    "                     'Away_Carolina', 'Home', 'Away_New_England','Home',\\\n",
    "                     'Home', 'Away_Buffalo', 'Away_Kansas_City','Home',\\\n",
    "                    'Home', 'Away_New_Jersey', 'Home', 'Away_New_England',\\\n",
    "                     'Away_Cincinatti','Home', 'Home','Away_Texans', \\\n",
    "                     'Home', 'Away_Green Bay', 'Away_Indy','Home',\\\n",
    "                     'Home', 'Away_Minn', 'Home', 'Away_Buffalo'],\n",
    "        'Opponent': ['Chargers', 'Jets', 'Saints','Titans', 'Falcons', 'Jets','Ravens','Raiders',\\\n",
    "                    'Panthers','Buccaneers','Patriots','Broncos','Patriots','Bills','Chiefs','Bills',\\\n",
    "                    'Titans','Jets','Raiders','Patriots','Bengals','Bears','Lions','Texans',\\\n",
    "                    'Jets','Packers','Colts','Bills','Patriots','Vikings','Jaguars','Bills'],\n",
    "        'Result':['Win','Loss','Loss','Win','Win', 'Win','Loss','Loss',\\\n",
    "                 'Loss','Loss','Loss','Win','Win','Loss','Loss','Loss',\\\n",
    "                 'Win','Win','Win','Loss','Loss','Win','Loss','Loss',\\\n",
    "                 'Win','Loss','Loss','Win','Win','Loss','Loss','Loss'],\n",
    "        'Score':['19-17','20-6','20-0','16-10','20-17','31-28','40-0','27-24',\\\n",
    "                '45-21','30-20','35-17','35-9','27-20','24-16','29-13','22-16',\\\n",
    "                '27-20','20-12','28-20','38-7','27-17','31-28','32-21','42-23',\\\n",
    "                '13-6','31-12','27-24','21-17','34-33','41-17','17-7','42-17']}\n",
    "\n",
    "away_game_url = ['https://www.wunderground.com/history/daily/us/ca/san-diego/date/2017-9-17',\\\n",
    "                'https://www.wunderground.com/history/daily/us/nj/east-rutherford/date/2017-9-24',\\\n",
    "                'https://www.wunderground.com/history/daily/us/nj/east-rutherford/date/2018-9-16',\\\n",
    "                'https://www.wunderground.com/history/daily/us/ga/atlanta/date/2017-10-15',\\\n",
    "                'https://www.wunderground.com/history/daily/us/md/baltimore/date/2017-10-26',\\\n",
    "                'https://www.wunderground.com/history/daily/us/nc/charlotte/date/2017-11-13',\\\n",
    "                'https://www.wunderground.com/history/daily/us/ma/foxborough/date/2017-11-26',\\\n",
    "                'https://www.wunderground.com/history/daily/us/ma/foxborough/date/2018-9-30',\\\n",
    "                'https://www.wunderground.com/history/daily/us/ny/buffalo/date/2017-12-17',\\\n",
    "                'https://www.wunderground.com/history/daily/us/ny/buffalo/date/2018-12-30',\\\n",
    "                'https://www.wunderground.com/history/daily/us/mo/kansas-city/date/2017-12-24',\\\n",
    "                'https://www.wunderground.com/history/daily/us/oh/cincinnati/date/2018-10-07',\\\n",
    "                'https://www.wunderground.com/history/daily/us/tx/houston/date/2018-10-25',\\\n",
    "                'https://www.wunderground.com/history/daily/us/wi/green-bay/date/2018-11-11',\\\n",
    "                'https://www.wunderground.com/history/daily/us/in/indianapolis/date/2018-11-25',\\\n",
    "                'https://www.wunderground.com/history/daily/us/mn/minneapolis/KMSP/date/2018-12-16']\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data = dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium uses double verify to make sure that it is a legitamate person using, i believe it is because of the ads on the page, they want to have legit impressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home and Away game dates for the past two seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dates = ['2017-10-01', '2017-10-08', '2017-10-22', '2017-11-05', '2017-11-19', '2017-12-03','2017-12-11',\\\n",
    "             '2017-12-31', '2018-9-09', '2018-9-23', '2018-10-14', '2018-10-21', '2018-11-04', '2018-12-02',\\\n",
    "             '2018-12-09','2018-12-23']\n",
    "away_dates = ['2017-9-17','2017-9-24','2018-9-16', '2017-10-15', '2017-10-26', '2017-11-13', '2017-11-26','2018-9-30',\\\n",
    "              '2017-12-17', '2018-12-30', '2017-12-24', '2018-10-07', '2018-10-25', '2018-11-11','2018-11-25','2018-12-16' ]\n",
    "\n",
    "backup_away = [72,78,76,74,51,53,44,62,22,30,21,77,65,29,49,39]\n",
    "backup_home_avg_list = [86,87,84,77,73,74,58,62,83,83 ,83,83,82,80,78,63]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create wunderground urls for home games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_home_urls():\n",
    "    url = 'https://www.wunderground.com/history/daily/us/fl/fort-lauderdale/KFLL/date/'\n",
    "    gameday_list = []\n",
    "    for date in home_dates:\n",
    "        gameday_url = url + date\n",
    "        gameday_list.append(gameday_url)\n",
    "    return gameday_list\n",
    "\n",
    "gameday_list = get_home_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium documentation: https://splinter.readthedocs.io/en/latest/drivers/chrome.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Selenium to automate a web browser\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def home_temp_scrape(url_list):\n",
    "    \"\"\"Scrapes wunderground selecting the daily average temperature for Home Games at HardRock Stadium.\"\"\"\n",
    "    avg_temp_list = []\n",
    "    for url in url_list:\n",
    "        browser.visit(url)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #Once i can parse the page, pull other attributes as well\n",
    "        try:\n",
    "            avg_temp = soup.find_all('tr')[3].find('td').text\n",
    "            avg_temp_list.append(avg_temp)\n",
    "        except: IndexError\n",
    "    browser.quit()\n",
    "    return avg_temp_list\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "home_temperatures = home_temp_scrape(gameday_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def away_game_scrape(url_list):\n",
    "    \"\"\"Scrapes wunderground selecting the average daily temperature for various away game locations on gamedays.\"\"\"\n",
    "    away_avg_temp_list=[]\n",
    "    for url in url_list:\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "            try:\n",
    "                avg_temp = soup.find_all('tr')[3].find('td').text\n",
    "                away_avg_temp_list.append(avg_temp)\n",
    "            except:IndexError\n",
    "    browser.quit()\n",
    "    return away_avg_temp_list\n",
    "\n",
    "                    \n",
    "away_temperatures = away_game_scrape(away_game_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_scrapes(home_weather, away_weather):\n",
    "     \"\"\"Evaluates whether the scrapes returned a full set of temperatures\"\"\"\n",
    "    if len(home_weather) <16:\n",
    "        home_weather = backup_home_avg_list\n",
    "    if len(away_weather) < 16:\n",
    "        away_weather = backup_away\n",
    "    return home_weather, away_weather\n",
    "\n",
    "#home_temperature, away_temperature = eval_scrapes(home_temperatures,away_temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_to_dataframe(home_date, home_weather, away_date, away_weather ,dataframe):\n",
    "    \"\"\"Adding the scraped temperatures to the dataframe\"\"\"\n",
    "    home_dict = dict(zip(home_date,home_weather))\n",
    "    away_dict = dict(zip(away_date,away_weather))\n",
    "\n",
    "    home_dict.update(away_dict)\n",
    "    dataframe['Temperature(Â°F)'] = dataframe['Dates'].map(home_dict)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "data = temp_to_dataframe(home_dates,backup_home_avg_list,away_dates,backup_away, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_dome():\n",
    "    \"\"\"Creates dataframe column indicating whether a team plays in a dome\"\"\"\n",
    "    plays_in_dome = ['Away_Atlanta','Away_Indy', 'Away_Minn', 'Away_Texans']  \n",
    "    data['Dome']= data.Location.isin(plays_in_dome)                            \n",
    "    return data\n",
    "\n",
    "data = has_dome()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = data.loc[data.Result =='Win']\n",
    "wins['Temperature'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = data.loc[data.Location == 'Home']\n",
    "home.Temperature.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.Temperature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting As a pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import table \n",
    "\n",
    "ax = plt.subplot(111, frame_on=False) # no visible frame\n",
    "ax.xaxis.set_visible(False)  # hide the x axis\n",
    "ax.yaxis.set_visible(False)  # hide the y axis\n",
    "\n",
    "table(ax, data, loc='center')\n",
    "\n",
    "plt.savefig('mytable.jpg',  bbox_inches='tight', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website : https://pyfpdf.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF, HTMLMixin\n",
    "\n",
    " \n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=22)\n",
    "pdf.cell(w=200, h= 15, txt=\"Miami Dolphins Weather Report\", ln=1, align=\"C\")\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt='Created by Nicholas Radich', ln=2,align= 'C')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt='Data acquired from www.wunderground.com', ln=3,align= 'C')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt='Findings of Weather Analysis:', ln=4,align= 'L')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt=\"The average temperature of Dolphin's game over the past two years was 64Â°F.\", ln=4,align= 'L')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt=\"The low temperature of Dolphin's game over the past two years was 21Â°F.\", ln=4,align= 'L')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt=\"The high temperature of Dolphin's game over the past two years was 87Â°F.\", ln=4,align= 'L')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt=\"The average temperature in Dolphins' victories was 78Â°F.\", ln=4,align= 'L')\n",
    "pdf.set_font('Arial', size =14)\n",
    "pdf.cell(w=200, h=10, txt=\"The average temperature in Dolphins' home games was 77.25Â°F.\", ln=4,align= 'L')\n",
    "pdf.image('./mytable.jpg',x =25, y=105,w=150, h=175)\n",
    "pdf.output(\"Dolphins_Weather_Report.pdf\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
